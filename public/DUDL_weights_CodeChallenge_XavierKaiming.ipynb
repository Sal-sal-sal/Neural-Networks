{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ZsHB4M1ATeF8PjMDHbSxOimxmUMmCfbM","timestamp":1619024940729},{"file_id":"1IuDazKJ8VD0FyCGt_hC-h3ZSPFypxDM0","timestamp":1618829295498},{"file_id":"1ZD_ADbh6qrlHE16V7Yc8VF9Vrn2c6bMQ","timestamp":1617088339885}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bhWV8oes-wKR"},"source":["# COURSE: A deep understanding of deep learning\n","## SECTION: Weight inits and investigations\n","### LECTURE: CodeChallenge: Xavier vs. Kaiming\n","#### TEACHER: Mike X Cohen, sincxpress.com\n","##### COURSE URL: udemy.com/course/dudl/?couponCode=202108"]},{"cell_type":"code","metadata":{"id":"7U3TmybM4yMw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736259008680,"user_tz":-300,"elapsed":340,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}},"outputId":"58604339-4caa-47e5-c025-35abf0e349a6"},"source":["### import libraries\n","\n","# for DL modeling\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader,TensorDataset\n","from sklearn.model_selection import train_test_split\n","\n","# for number-crunching\n","import numpy as np\n","import scipy.stats as stats\n","\n","# for dataset management and bonus visualization\n","import pandas as pd\n","import seaborn as sns\n","\n","# for data visualization\n","import matplotlib.pyplot as plt\n","from IPython import display\n","display.set_matplotlib_formats('svg')"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-5cdf320a287a>:21: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n","  display.set_matplotlib_formats('svg')\n"]}]},{"cell_type":"markdown","metadata":{"id":"2anVFzBXGdwH"},"source":["# Import and process the data"]},{"cell_type":"code","metadata":{"id":"0ohXIxzt4_U2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736259009347,"user_tz":-300,"elapsed":316,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}},"outputId":"3d311e14-ac4d-44f0-d737-f48f097bf4b0"},"source":["# import the data\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n","data = pd.read_csv(url,sep=';')\n","data = data[data['total sulfur dioxide']<200] # drop a few outliers\n","\n","# z-score all columns except for quality\n","cols2zscore = data.keys()\n","cols2zscore = cols2zscore.drop('quality')\n","data[cols2zscore] = data[cols2zscore].apply(stats.zscore)\n","\n","# create a new column for binarized (boolean) quality\n","data['boolQuality'] = 0\n","# data['boolQuality'][data['quality']<6] = 0 # implicit in the code! just here for clarity\n","data['boolQuality'][data['quality']>5] = 1"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-6f5cb70bcde3>:14: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n","You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n","A typical example is when you are setting values in a column of a DataFrame, like:\n","\n","df[\"col\"][row_indexer] = value\n","\n","Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","\n","  data['boolQuality'][data['quality']>5] = 1\n","<ipython-input-12-6f5cb70bcde3>:14: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  data['boolQuality'][data['quality']>5] = 1\n"]}]},{"cell_type":"code","metadata":{"id":"2kZ6YPe8Gav5","executionInfo":{"status":"ok","timestamp":1736259009348,"user_tz":-300,"elapsed":6,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}}},"source":["# convert from pandas dataframe to tensor\n","dataT  = torch.tensor( data[cols2zscore].values ).float()\n","labels = torch.tensor( data['boolQuality'].values ).float()\n","labels = labels[:,None] # transform to matrix\n","\n","# use scikitlearn to split the data\n","train_data,test_data, train_labels,test_labels = train_test_split(dataT, labels, test_size=.1)\n","\n","# then convert them into PyTorch Datasets (note: already converted to tensors)\n","train_dataDataset = TensorDataset(train_data,train_labels)\n","test_dataDataset  = TensorDataset(test_data,test_labels)\n","\n","# finally, create dataloaders\n","train_loader = DataLoader(train_dataDataset,batch_size=32, shuffle=True, drop_last=True)\n","test_loader  = DataLoader(test_dataDataset,batch_size=test_dataDataset.tensors[0].shape[0])"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7g0mivk5GqP"},"source":["# Now for the DL part"]},{"cell_type":"code","metadata":{"id":"N0vAnQi9DNRa","executionInfo":{"status":"ok","timestamp":1736259009348,"user_tz":-300,"elapsed":6,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}}},"source":["# create a class for the model\n","\n","class ANNwine(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    ### input layer\n","    self.input = nn.Linear(11,16)\n","\n","    ### hidden layers\n","    self.fc1 = nn.Linear(16,32)\n","    self.fc2 = nn.Linear(32,32)\n","\n","    ### output layer\n","    self.output = nn.Linear(32,1)\n","\n","  # forward pass\n","  def forward(self,x):\n","    x = F.relu( self.input(x) )\n","    x = F.relu( self.fc1(x) )\n","    x = F.relu( self.fc2(x) )\n","    return self.output(x)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IuCixgNfDMZS"},"source":["# A function to train the model"]},{"cell_type":"code","metadata":{"id":"0XRPe56rGp2k","executionInfo":{"status":"ok","timestamp":1736259204886,"user_tz":-300,"elapsed":318,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}}},"source":["# global parameter\n","numepochs = 20\n","\n","def trainTheModel(winenet):\n","\n","  # loss function and optimizer\n","  lossfun = nn.BCEWithLogitsLoss()\n","  optimizer = torch.optim.Adam(winenet.parameters(),lr=.01)\n","\n","  # initialize losses\n","  losses   = torch.zeros(numepochs)\n","  trainAcc = []\n","  testAcc  = []\n","\n","  # loop over epochs\n","  for epochi in range(numepochs):\n","\n","    # loop over training data batches\n","    winenet.train()\n","    batchAcc  = []\n","    batchLoss = []\n","    for X,y in train_loader:\n","\n","      # forward pass and loss\n","      yHat = winenet(X)\n","      loss = lossfun(yHat,y)\n","\n","      # backprop\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # loss from this batch\n","      batchLoss.append(loss.item())\n","\n","      # compute training accuracy for this batch\n","      batchAcc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n","    # end of batch loop...\n","\n","    # now that we've trained through the batches, get their average training accuracy\n","    trainAcc.append( np.mean(batchAcc) )\n","\n","    # and get average losses across the batches\n","    losses[epochi] = np.mean(batchLoss)\n","\n","    # test accuracy\n","    winenet.eval()\n","    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n","    with torch.no_grad(): # deactivates autograd\n","      yHat = winenet(X)\n","    testAcc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n","\n","  # function output\n","  return trainAcc,testAcc,losses"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T_JKCpfe_CfC"},"source":["# Experiment: initialize weights and test model!"]},{"cell_type":"code","metadata":{"id":"oL7EqhYjGp51","colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"status":"error","timestamp":1736259205341,"user_tz":-300,"elapsed":3,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}},"outputId":"91a45105-2b8b-4114-b56d-d48e1f9ef2d8"},"source":["# create a model\n","winenet_xavier = ANNwine()\n","\n","# change the weights (leave biases as Kaiming [default])\n","for p in winenet_xavier.named_parameters():\n","  if 'weight' in p[0]:\n","    nn.init.xavier_normal_(p[1].data)\n","\n","# train the model and record its output\n","trainAcc_X,testAcc_X,losses_X = trainTheModel(winenet_xavier)"],"execution_count":27,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'sympy.printing' has no attribute 'str'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-7a74975bee02>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# train the model and record its output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainAcc_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestAcc_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainTheModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinenet_xavier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-26-66fac0202ce9>\u001b[0m in \u001b[0;36mtrainTheModel\u001b[0;34m(winenet)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# loss function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mlossfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# initialize losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dynamo_disable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdisable_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompileTimeInstructionCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minductor_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   5526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5528\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_PythonPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrPrinter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5529\u001b[0m     \"\"\"\n\u001b[1;32m   5530\u001b[0m     \u001b[0mUtil\u001b[0m \u001b[0mprinter\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mreplaces\u001b[0m \u001b[0msympy\u001b[0m \u001b[0msymbols\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'sympy.printing' has no attribute 'str'"]}]},{"cell_type":"code","metadata":{"id":"otCYJ2ambF2i","executionInfo":{"status":"aborted","timestamp":1736259009348,"user_tz":-300,"elapsed":4,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}}},"source":["# create and train a model\n","winenet_kaiming = ANNwine()\n","\n","# change the weights (leave biases as Kaiming [default])\n","for p in winenet_kaiming.named_parameters():\n","  if 'weight' in p[0]:\n","    nn.init.kaiming_uniform_(p[1].data,nonlinearity='relu')\n","\n","# train the model and record its output\n","trainAcc_K,testAcc_K,losses_K = trainTheModel(winenet_kaiming)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVjJayfhbTWH","executionInfo":{"status":"aborted","timestamp":1736259009348,"user_tz":-300,"elapsed":4,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}}},"source":["# plot the results\n","\n","fig,ax = plt.subplots(1,3,figsize=(18,4))\n","\n","# losses\n","ax[0].plot(losses_X,label='Xavier')\n","ax[0].plot(losses_K,label='Kaiming')\n","ax[0].set_title('Loss')\n","\n","# train accuracy\n","ax[1].plot(trainAcc_X,label='Xavier')\n","ax[1].plot(trainAcc_K,label='Kaiming')\n","ax[1].set_ylabel('Accuracy (%)')\n","ax[1].set_title('TRAIN')\n","\n","# test accuracy\n","ax[2].plot(testAcc_X,label='Xavier')\n","ax[2].plot(testAcc_K,label='Kaiming')\n","ax[2].set_ylabel('Accuracy (%)')\n","ax[2].set_title('TEST')\n","\n","for i in range(3):\n","  ax[i].legend()\n","  ax[i].grid('on')\n","  ax[i].set_xlabel('Epochs')\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_7mAB4utDMeh"},"source":["# Repeat the experiment to get more stable results"]},{"cell_type":"code","metadata":{"id":"_UMTFQTPqBjr","executionInfo":{"status":"aborted","timestamp":1736259009348,"user_tz":-300,"elapsed":4,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}}},"source":["# Number of experiment iterations (note: ~1 min/iteration)\n","numExps = 10\n","\n","\n","# dimensions of results:\n","#  1 - experiment run\n","#  2 - metric (loss/train/test)\n","#  3 - weight init (X/K)\n","results = np.zeros( (numExps,3,2) )\n","\n","\n","for expi in range(numExps):\n","\n","  ### XAVIER\n","  # create a model and change the weights\n","  winenet_xavier = ANNwine()\n","  for p in winenet_xavier.named_parameters():\n","    if 'weight' in p[0]:\n","      nn.init.xavier_normal_(p[1].data)\n","\n","  # train the model and record its output\n","  trainAcc_X,testAcc_X,losses_X = trainTheModel(winenet_xavier)\n","\n","\n","  ### KAIMING\n","  # create a model and change the weights\n","  winenet_kaiming = ANNwine()\n","  for p in winenet_kaiming.named_parameters():\n","    if 'weight' in p[0]:\n","      nn.init.kaiming_uniform_(p[1].data,nonlinearity='relu')\n","\n","  # train the model and record its output\n","  trainAcc_K,testAcc_K,losses_K = trainTheModel(winenet_kaiming)\n","\n","\n","  ### collect the results!\n","  results[expi,0,0] = torch.mean(losses_X[-5:]).item()\n","  results[expi,0,1] = torch.mean(losses_K[-5:]).item()\n","\n","  results[expi,1,0] = np.mean(trainAcc_X[-5:])\n","  results[expi,1,1] = np.mean(trainAcc_K[-5:])\n","\n","  results[expi,2,0] = np.mean(testAcc_X[-5:])\n","  results[expi,2,1] = np.mean(testAcc_K[-5:])\n","\n","\n","  # a little update message...\n","  print(f'Finished run {expi}/{numExps}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VbOnVLUwv4h","executionInfo":{"status":"aborted","timestamp":1736259009348,"user_tz":-300,"elapsed":4,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}}},"source":["# And plot the results\n","fig,ax = plt.subplots(1,3,figsize=(15,4))\n","\n","# plot titles\n","metric = ['Loss','Train acc.','Test acc.']\n","\n","for i in range(3):\n","\n","  # plot the results\n","  ax[i].plot(np.zeros(numExps),results[:,i,0],'bo')\n","  ax[i].plot(np.ones(numExps),results[:,i,1],'ro')\n","\n","  # run a t-test to formalize the comparison\n","  t,p = stats.ttest_ind(results[:,i,0],results[:,i,1])\n","  title = '%s (t=%.2f, p=%.3f)' %(metric[i],t,p)\n","\n","\n","  # make the plot look a bit nicer\n","  ax[i].set_xlim([-1,2])\n","  ax[i].set_xticks([0,1])\n","  ax[i].set_xticklabels(['Xavier','Kaiming'])\n","  ax[i].set_title(title)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b-DYWPmfTfZR"},"source":["# Additional explorations"]},{"cell_type":"code","metadata":{"id":"XRokrXL9Thv7","executionInfo":{"status":"aborted","timestamp":1736259009348,"user_tz":-300,"elapsed":4,"user":{"displayName":"Ccg Tru","userId":"12490002955896674145"}}},"source":["# 1) Adam usually works better than SGD with fewer training epochs. Does Adam also equalize the differences attributable\n","#    to weight initialization?\n","#\n","# 2) The discrepancy between training and test performance suggests that Kaiming initialization involved some overfitting.\n","#    What are some strategies you could employ to reduce overfitting?\n","#\n","# 3) The difference between X and K initialization is likely to increase with more weights. Change the number of units in\n","#    the hidden layers from 32 to 64.\n","#"],"execution_count":null,"outputs":[]}]}